{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Projeto Hillary x Trump\n",
    "\n",
    "Nesse projeto vamos utilizar tweets relacionados a última eleição presidencial dos Estados Unidos, onde Hillary Clinton e Donald Trump dispuram o pleito final. A proposta é utilizar os métodos de aprendizados supervisionados estudados para classificar tweets entre duas categorias: Hillary e Trump. \n",
    "\n",
    "O primeiro passo foi obter um conjunto de tweets que foi publicado pelas contas oficiais do tweet dos dois candidatos. Para isso, vamos utilizar [este dataset](https://www.kaggle.com/benhamner/clinton-trump-tweets) disponibilizado pelo Kaglle. Com este conjunto de dados, vamos construir um modelo capaz de aprender, a partir de um conjunto de palavras, se o texto foi digitado pela conta da Hillary ou do Trump. \n",
    "\n",
    "Uma vez que este modelo foi construído, vamos classificar um conjunto novo de dados relacionados às eleições americadas e classifica-los em um dos discursos. A proposta é tentar classificar tweets que tenham um direcionamento mais próximo do discurso da Hillary e aqueles que são mais próximos do discurso do Trump. Lê-se como discurso os tweets publicados.\n",
    "\n",
    "Para essa base de teste, vamos utilizar um subconjunto de tweets [deste dataset](https://github.com/chrisalbon/election_day_2016_twitter) que consta com tweets que foram postados no dia da eleição americana. \n",
    "\n",
    "Para exibir os resultados, vamos construir uma página html. Além da análise automática, esta página terá informações sobre os termos mais citados pelas contas dos candidatos. \n",
    "\n",
    "Sendo assim, o primeiro passo é gerar a base de tweets dos candidatos e extrair as informações mais relevantes. Vamos trabalhar primeiro aqui no Jupyter Notebook para testar os métodos. Ao final será gerado um JSON que será lido pela página HTML. Um exemplo da página já alimentada pode ser encontrada [neste link](http://adolfoguimaraes.pythonanywhere.com/machinelearning/hillarytrump).\n",
    "\n",
    "Vamos começar ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pré-Processamento da Base dos Candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "df = pd.read_csv('../datasets/hillary_trump/tweets.csv')\n",
    "dataset = df[['text','handle']]\n",
    "dict_ = dataset.T.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "O objeto **dict\\_** representa todos os textos associados a classe correspondente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Na etapa de pré-processamento vamos fazer algumas operações: \n",
    "\n",
    "* retirar dos textos hashtags, usuários e links. Essas informações serão incluídas em listas separadas paara serem usadas posteriormente. \n",
    "* serão eliminados stopwords, símbolos de pontuação, palavras curtas;\n",
    "* numerais tambéms serão descartados, mantendo apenas palavras. \n",
    "\n",
    "Essas etapas de pré-processamento dependem do objetivo do trabalho. Pode ser de interessa, a depender da tarefa de classificação, manter tais símbolos. Para o nosso trabalho, só é de interesse as palavras em si.\n",
    "\n",
    "Para esta tarefa, vamos utilizar também o NLTK, conjunto de ferramentas voltadas para o processamento de linguagem natural. \n",
    "\n",
    "vamos criar um método para isso, já que iremos utiliza-lo mais adiante com a base de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from unicodedata import normalize, category\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from collections import Counter, Set\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def pre_process_text(text):\n",
    "    \n",
    "    \n",
    "    # Expressão regular para extrair padrões do texto. São reconhecidos (na ordem, o símbolo | separa um padrão):\n",
    "    # - links com https, links com http, links com www, palavras, nome de usuários (começa com @), hashtags (começa com #)\n",
    "    pattern = r'(https://[^\"\\' ]+|www.[^\"\\' ]+|http://[^\"\\' ]+|[a-zA-Z]+|\\@\\w+|\\#\\w+)'\n",
    "    \n",
    "    #Cria a lista de stopwords\n",
    "    english_stop = stopwords.words(['english'])\n",
    "    \n",
    "    users_cited = []\n",
    "    hash_tags = []\n",
    "    tokens = []\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "    users_cited = [e for e in patterns if e[0] == '@']\n",
    "    hashtags = [e for e in patterns if e[0] == '#']\n",
    "    \n",
    "    \n",
    "    tokens = [e for e in patterns if e[:4] != 'http']\n",
    "    tokens = [e for e in tokens if e[:4] != 'www.']\n",
    "    tokens = [e for e in tokens if e[0] != '#']\n",
    "    tokens = [e for e in tokens if e[0] != '@']\n",
    "    tokens = [e for e in tokens if e not in english_stop]\n",
    "    tokens = [e for e in tokens if len(e) > 3]\n",
    "    \n",
    "    return users_cited, hashtags, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "users_cited_h = []  # armazena os usuários citatdos por hillary\n",
    "users_cited_t = []  # armazena os usuários citados por trump\n",
    "\n",
    "hashtags_h = [] # armazena as hashtags de hillary\n",
    "hashtags_t = [] # armazena as hashtags de trump\n",
    "\n",
    "words_h = [] # lista de palavras que apareceram no discurso de hillary\n",
    "words_t = [] # lista de palavras que apareceram no discurso de trump\n",
    "\n",
    "all_tokens = [] # armazena todos os tokens, para gerar o vocabulário final\n",
    "all_texts = [] # armazena todos \n",
    "\n",
    "for d in dict_:\n",
    "    \n",
    "    text_ =  dict_[d][0]\n",
    "    class_ = dict_[d][1]\n",
    "    \n",
    "    users_, hash_, tokens_ = pre_process_text(text_)\n",
    "    \n",
    "    if class_ == \"HillaryClinton\":\n",
    "        class_ = \"hillary\"\n",
    "        users_cited_h += users_\n",
    "        hashtags_h += hash_\n",
    "        words_h += tokens_\n",
    "        \n",
    "    elif class_ == \"realDonaldTrump\":\n",
    "        class_ = \"trump\"\n",
    "        users_cited_t += users_\n",
    "        hashtags_t += hash_\n",
    "        words_t += tokens_\n",
    "\n",
    "    \n",
    "    temp_dict = {\n",
    "        'text': \" \".join(tokens_),\n",
    "        'class_': class_ \n",
    "    }\n",
    "    \n",
    "    all_tokens += tokens_\n",
    "    all_texts.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termos mais frequentes ditos por Hillary:\n",
      "\n",
      "trump\n",
      "hillary\n",
      "donald\n",
      "president\n",
      "america\n",
      "people\n",
      "make\n",
      "families\n",
      "women\n",
      "need\n"
     ]
    }
   ],
   "source": [
    "print(\"Termos mais frequentes ditos por Hillary:\")\n",
    "print()\n",
    "hillary_frequent_terms = nltk.FreqDist(words_h).most_common(10)\n",
    "\n",
    "for word in hillary_frequent_terms:\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termos mais frequentes ditos por Trump:\n",
      "\n",
      "thank\n",
      "great\n",
      "trump\n",
      "hillary\n",
      "people\n",
      "america\n",
      "clinton\n",
      "cruz\n",
      "crooked\n",
      "make\n"
     ]
    }
   ],
   "source": [
    "print(\"Termos mais frequentes ditos por Trump:\")\n",
    "print()\n",
    "trump_frequent_terms = nltk.FreqDist(words_t).most_common(10)\n",
    "\n",
    "for word in trump_frequent_terms:\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bigrams e Trigram mais frequentes da Hillary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump\n",
      "hillary clinton\n",
      "make sure\n",
      "united states\n",
      "commander chief\n",
      "president united states\n",
      "hillary donald trump\n",
      "watch live hillary\n",
      "donald trump says\n",
      "president donald trump\n"
     ]
    }
   ],
   "source": [
    "#Pegando os bigram e trigram mais frequentes\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(words_h)\n",
    "tcf = TrigramCollocationFinder.from_words(words_h)\n",
    "\n",
    "bcf.apply_freq_filter(3)\n",
    "tcf.apply_freq_filter(3)\n",
    "\n",
    "result_bi = bcf.nbest(BigramAssocMeasures.raw_freq, 5)\n",
    "result_tri = tcf.nbest(TrigramAssocMeasures.raw_freq, 5)\n",
    "\n",
    "\n",
    "hillary_frequent_bitrigram = []\n",
    "\n",
    "for r in result_bi:\n",
    "    w_ = \" \".join(r)\n",
    "    print(w_)\n",
    "    hillary_frequent_bitrigram.append(w_)\n",
    "    \n",
    "print\n",
    "for r in result_tri:\n",
    "    w_ = \" \".join(r)\n",
    "    print(w_)\n",
    "    hillary_frequent_bitrigram.append(w_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bigrams e Trigram mais frequentes do Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crooked hillary\n",
      "hillary clinton\n",
      "make america\n",
      "america great\n",
      "donald trump\n",
      "make america great\n",
      "crooked hillary clinton\n",
      "make america safe\n",
      "goofy elizabeth warren\n",
      "america safe great\n"
     ]
    }
   ],
   "source": [
    "bcf = BigramCollocationFinder.from_words(words_t)\n",
    "tcf = TrigramCollocationFinder.from_words(words_t)\n",
    "\n",
    "bcf.apply_freq_filter(3)\n",
    "tcf.apply_freq_filter(3)\n",
    "\n",
    "result_bi = bcf.nbest(BigramAssocMeasures.raw_freq, 5)\n",
    "result_tri = tcf.nbest(TrigramAssocMeasures.raw_freq, 5)\n",
    "\n",
    "\n",
    "trump_frequent_bitrigram = []\n",
    "\n",
    "\n",
    "for r in result_bi:\n",
    "    w_ = \" \".join(r)\n",
    "    print(w_)\n",
    "    trump_frequent_bitrigram.append(w_)\n",
    "print\n",
    "for r in result_tri:\n",
    "    w_ = \" \".join(r)\n",
    "    print(w_)\n",
    "    trump_frequent_bitrigram.append(w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Construindo um *bag of words* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cada token é concatenado em uma única string que representa um tweet\n",
    "# Cada classe é atribuída a um vetor (hillary, trump)\n",
    "\n",
    "# Instâncias: [t1, t2, t3, t4]\n",
    "# Classes: [c1, c2, c3, c4]\n",
    "\n",
    "all_tweets = []\n",
    "all_class = []\n",
    "\n",
    "for t in all_texts:\n",
    "    all_tweets.append(t['text'])\n",
    "    all_class.append(t['class_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criar o bag of words...\n",
      "\n",
      "Train data: OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"Criar o bag of words...\\n\")\n",
    "\n",
    "#Número de features, coluna da tabela\n",
    "max_features = 2000\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = max_features) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "X = vectorizer.fit_transform(all_tweets)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "X = X.toarray()\n",
    "y = all_class\n",
    "\n",
    "print(\"Train data: OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6444, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ao final deste processo já temos nossa base de dados dividido em duas variáveis: X e y. \n",
    "\n",
    "**X** corresponde ao *bag of words*, ou seja, cada linha consiste de um twitter e cada coluna de uma palavra presente no vocabulário da base dados. Para cada linha/coluna é atribuído um valor que corresponde a quantidade de vezes que aquela palavra aparece no respectivo tweet. Se a palavra não está presente, o valor de 0 é atribuído.\n",
    "\n",
    "**y** corresponde a classe de cada tweet: **hillary**, tweet do perfil **@HillaryClinton** e **trump**, tweet do perfil **@realDonaldTrump**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testando diferentes modelos\n",
    "\n",
    "Vamos testar os diferentes modelos estudados com a base de dados criada e escolher aquele que melhor generaliza o conjunto de dados. Para testar os modelos, vamos utilizar validação cruzada de 10 folds. Aplique os modelos estudados: \n",
    "\n",
    "* KNN (teste diferentes valores de K e escolha o melhor)\n",
    "* Árvore de Decisão\n",
    "* SVM (varie o valor de C e escolha o melhor)\n",
    "\n",
    "Além disso, teste dois outros modelos: \n",
    "\n",
    "* RandomForest \n",
    "* Naive Bayes\n",
    "\n",
    "Para estes dois, pesquise no Google como utiliza-los no Scikit-Learn.\n",
    "\n",
    "Para cada modelo imprima a acurácia no treino e na média dos 10 folds da validação cruzada. A escolha do melhor deve ser feita a partir do valor da média da validação cruzada. \n",
    "\n",
    "O melhor modelo será utilizado para classificar outros textos extraídos do twitter e na implementação da página web. \n",
    "\n",
    "***Atenção: dada a quantidade de dados, alguns modelos pode demorar alguns minutos para executar***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Teste os modelos a partir daqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Usando o melhor modelo em novos textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vamos executar o melhor clasificador em um conjunto de textos novos. Esses textos não tem classificação. Eles foram postados durante o dia da eleição americana. A idéia é identificar de forma automática os tweets que estão mais próximos dos discursos da Hillary Clinton e de Donald Trump. \n",
    "\n",
    "\n",
    "*** Essa tarefa será realizada em sala após a entrega da atividade do melhor modelo ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gerando o .json lido pela página web\n",
    "\n",
    "*** Esse será o JSON gerado após a etapa de teste do melhor modelo. Essa tarefa também será realizada em sala após a entrega do teste dos melhores modelos. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hillary_frequent_hashtags = nltk.FreqDist(hashtags_h).most_common(10)\n",
    "trump_frequent_hashtags = nltk.FreqDist(hashtags_t).most_common(10)\n",
    "\n",
    "dict_web = {\n",
    "    'hillary_information': {\n",
    "        'frequent_terms': hillary_frequent_terms,\n",
    "        'frequent_bitrigram': hillary_frequent_bitrigram,\n",
    "        'frequent_hashtags': hillary_frequent_hashtags\n",
    "    },\n",
    "    'trump_information': {\n",
    "        'frequent_terms': trump_frequent_terms,\n",
    "        'frequent_bitrigram': trump_frequent_bitrigram,\n",
    "        'frequent_hashtags': trump_frequent_hashtags\n",
    "    },\n",
    "    'classified_information': {\n",
    "        'hillary_terms': hillary_classified_frequent_terms,\n",
    "        'hillary_bigram': hillary_classified_bitrigram,\n",
    "        'trump_terms': trump_classified_frequent_terms,\n",
    "        'trump_bigram': trump_classified_bitrigram,\n",
    "        'texts_classified': all_classified\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data.json', 'w') as outfile:  \n",
    "    json.dump(dict_web, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
